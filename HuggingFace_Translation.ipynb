{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c381a144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9f0d4919ce424db964107aa979d643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.40k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c24ca62a3eb40eabd5d4b940724663c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.98k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset opus_books/en-fr (download: 11.45 MiB, generated: 31.47 MiB, post-processed: Unknown size, total: 42.92 MiB) to /home/xyin/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0e55cc4fe3452a8ce11ecf8c0785af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset opus_books downloaded and prepared to /home/xyin/.cache/huggingface/datasets/opus_books/en-fr/1.0.0/e8f950a4f32dc39b7f9088908216cd2d7e21ac35f893d04d39eb594746af2daf. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379186aa42ee4d8991fc26d7e46f4741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "books = load_dataset(\"opus_books\", \"en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fe587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = books[\"train\"].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47112d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56043',\n",
       " 'translation': {'en': \"It was, in truth, that redoubtable Cour des Miracles, whither an honest man had never penetrated at such an hour; the magic circle where the officers of the Châtelet and the sergeants of the provostship, who ventured thither, disappeared in morsels; a city of thieves, a hideous wart on the face of Paris; a sewer, from which escaped every morning, and whither returned every night to crouch, that stream of vices, of mendicancy and vagabondage which always overflows in the streets of capitals; a monstrous hive, to which returned at nightfall, with their booty, all the drones of the social order; a lying hospital where the bohemian, the disfrocked monk, the ruined scholar, the ne'er−do−wells of all nations, Spaniards, Italians, Germans,−−of all religions, Jews, Christians, Mahometans, idolaters, covered with painted sores, beggars by day, were transformed by night into brigands; an immense dressing−room, in a word, where, at that epoch, the actors of that eternal comedy, which theft, prostitution, and murder play upon the pavements of Paris, dressed and undressed.\",\n",
       "  'fr': 'Il était en effet dans cette redoutable Cour des Miracles, où jamais honnête homme n’avait pénétré à pareille heure ; cercle magique où les officiers du Châtelet et les sergents de la prévôté qui s’y aventuraient disparaissaient en miettes ; cité des voleurs, hideuse verrue à la face de Paris ; égout d’où s’échappait chaque matin, et où revenait croupir chaque nuit ce ruisseau de vices, de mendicité et de vagabondage toujours débordé dans les rues des capitales ; ruche monstrueuse où rentraient le soir avec leur butin tous les frelons de l’ordre social ; hôpital menteur où le bohémien, le moine défroqué, l’écolier perdu, les vauriens de toutes les nations, espagnols, italiens, allemands, de toutes les religions, juifs, chrétiens, mahométans, idolâtres, couverts de plaies fardées, mendiants le jour, se transfiguraient la nuit en brigands ; immense vestiaire, en un mot, où s’habillaient et se déshabillaient à cette époque tous les acteurs de cette comédie éternelle que le vol, la prostitution et le meurtre jouent sur le pavé de Paris.'}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1590cf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cd9694f8bb64e3cab2599b07700c46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22b7d59977642ab89fe6e6fc18cf644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b7f61d64b24374bc2cffe7e43c5896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyin/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f251fd4d0dc4b54a28cfc2630b8bbcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/102 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a64e83ecc37462ab157284a6e0eb589",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "source_lang = \"en\"\n",
    "target_lang = \"fr\"\n",
    "prefix = \"translate English to French: \"\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + example[source_lang] for example in examples[\"translation\"]]\n",
    "    targets = [example[target_lang] for example in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_books = books.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7ee30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "492fc0eb55d84166a2ef05e08ed50c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/231M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6370a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "558f8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/xyin/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 101668\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6355' max='6355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6355/6355 10:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.882000</td>\n",
       "      <td>1.665164</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./results/checkpoint-500\n",
      "Configuration saved in ./results/checkpoint-500/config.json\n",
      "Model weights saved in ./results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-1500\n",
      "Configuration saved in ./results/checkpoint-1500/config.json\n",
      "Model weights saved in ./results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-2500\n",
      "Configuration saved in ./results/checkpoint-2500/config.json\n",
      "Model weights saved in ./results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-3500\n",
      "Configuration saved in ./results/checkpoint-3500/config.json\n",
      "Model weights saved in ./results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-4500\n",
      "Configuration saved in ./results/checkpoint-4500/config.json\n",
      "Model weights saved in ./results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-5500\n",
      "Configuration saved in ./results/checkpoint-5500/config.json\n",
      "Model weights saved in ./results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, translation. If id, translation are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 25417\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6355, training_loss=1.935859407654529, metrics={'train_runtime': 645.5193, 'train_samples_per_second': 157.498, 'train_steps_per_second': 9.845, 'total_flos': 2501862985826304.0, 'train_loss': 1.935859407654529, 'epoch': 1.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_books[\"train\"],\n",
    "    eval_dataset=tokenized_books[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35196b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Layer (type:depth-idx)                                  Param #\n",
      "================================================================================\n",
      "T5ForConditionalGeneration                              --\n",
      "├─Embedding: 1-1                                        16,449,536\n",
      "├─T5Stack: 1-2                                          16,449,536\n",
      "│    └─Embedding: 2-1                                   (recursive)\n",
      "│    └─ModuleList: 2-2                                  --\n",
      "│    │    └─T5Block: 3-1                                3,147,008\n",
      "│    │    └─T5Block: 3-2                                3,146,752\n",
      "│    │    └─T5Block: 3-3                                3,146,752\n",
      "│    │    └─T5Block: 3-4                                3,146,752\n",
      "│    │    └─T5Block: 3-5                                3,146,752\n",
      "│    │    └─T5Block: 3-6                                3,146,752\n",
      "│    └─T5LayerNorm: 2-3                                 512\n",
      "│    └─Dropout: 2-4                                     --\n",
      "├─T5Stack: 1-3                                          16,449,536\n",
      "│    └─Embedding: 2-5                                   (recursive)\n",
      "│    └─ModuleList: 2-6                                  --\n",
      "│    │    └─T5Block: 3-7                                4,196,096\n",
      "│    │    └─T5Block: 3-8                                4,195,840\n",
      "│    │    └─T5Block: 3-9                                4,195,840\n",
      "│    │    └─T5Block: 3-10                               4,195,840\n",
      "│    │    └─T5Block: 3-11                               4,195,840\n",
      "│    │    └─T5Block: 3-12                               4,195,840\n",
      "│    └─T5LayerNorm: 2-7                                 512\n",
      "│    └─Dropout: 2-8                                     --\n",
      "├─Linear: 1-4                                           16,449,536\n",
      "================================================================================\n",
      "Total params: 60,506,624\n",
      "Trainable params: 60,506,624\n",
      "Non-trainable params: 0\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "print(summary(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b31b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "ResNet                                   [1, 1000]                 --\n",
      "├─Conv2d: 1-1                            [1, 64, 112, 112]         9,408\n",
      "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         128\n",
      "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
      "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
      "├─Sequential: 1-5                        [1, 256, 56, 56]          --\n",
      "│    └─Bottleneck: 2-1                   [1, 256, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           4,096\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           36,864\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          16,384\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 256, 56, 56]          512\n",
      "│    │    └─Sequential: 3-9              [1, 256, 56, 56]          16,896\n",
      "│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n",
      "│    └─Bottleneck: 2-2                   [1, 256, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-11                 [1, 64, 56, 56]           16,384\n",
      "│    │    └─BatchNorm2d: 3-12            [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-13                   [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-14                 [1, 64, 56, 56]           36,864\n",
      "│    │    └─BatchNorm2d: 3-15            [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-16                   [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-17                 [1, 256, 56, 56]          16,384\n",
      "│    │    └─BatchNorm2d: 3-18            [1, 256, 56, 56]          512\n",
      "│    │    └─ReLU: 3-19                   [1, 256, 56, 56]          --\n",
      "│    └─Bottleneck: 2-3                   [1, 256, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-20                 [1, 64, 56, 56]           16,384\n",
      "│    │    └─BatchNorm2d: 3-21            [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-22                   [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-23                 [1, 64, 56, 56]           36,864\n",
      "│    │    └─BatchNorm2d: 3-24            [1, 64, 56, 56]           128\n",
      "│    │    └─ReLU: 3-25                   [1, 64, 56, 56]           --\n",
      "│    │    └─Conv2d: 3-26                 [1, 256, 56, 56]          16,384\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 256, 56, 56]          512\n",
      "│    │    └─ReLU: 3-28                   [1, 256, 56, 56]          --\n",
      "├─Sequential: 1-6                        [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-4                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-29                 [1, 128, 56, 56]          32,768\n",
      "│    │    └─BatchNorm2d: 3-30            [1, 128, 56, 56]          256\n",
      "│    │    └─ReLU: 3-31                   [1, 128, 56, 56]          --\n",
      "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-35                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-36            [1, 512, 28, 28]          1,024\n",
      "│    │    └─Sequential: 3-37             [1, 512, 28, 28]          132,096\n",
      "│    │    └─ReLU: 3-38                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-5                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-39                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-40            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-41                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-42                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-43            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-44                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-45                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-46            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-47                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-6                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-48                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-49            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-50                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-51                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-52            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-53                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-54                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-55            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-56                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-7                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-57                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-58            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-59                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-60                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-61            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-62                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-63                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-64            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-65                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-8                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-66                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-67            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-68                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-69                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-70            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-71                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-72                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-73            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-74                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-9                   [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-75                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-76            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-77                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-78                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-79            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-80                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-81                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-82            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-83                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-10                  [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-84                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-85            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-86                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-87                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-88            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-89                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-90                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-91            [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-92                   [1, 512, 28, 28]          --\n",
      "│    └─Bottleneck: 2-11                  [1, 512, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-93                 [1, 128, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-94            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-95                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-96                 [1, 128, 28, 28]          147,456\n",
      "│    │    └─BatchNorm2d: 3-97            [1, 128, 28, 28]          256\n",
      "│    │    └─ReLU: 3-98                   [1, 128, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-99                 [1, 512, 28, 28]          65,536\n",
      "│    │    └─BatchNorm2d: 3-100           [1, 512, 28, 28]          1,024\n",
      "│    │    └─ReLU: 3-101                  [1, 512, 28, 28]          --\n",
      "├─Sequential: 1-7                        [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-12                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-102                [1, 256, 28, 28]          131,072\n",
      "│    │    └─BatchNorm2d: 3-103           [1, 256, 28, 28]          512\n",
      "│    │    └─ReLU: 3-104                  [1, 256, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-105                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-106           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-107                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-108                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-109           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─Sequential: 3-110            [1, 1024, 14, 14]         526,336\n",
      "│    │    └─ReLU: 3-111                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-13                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-112                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-113           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-114                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-115                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-116           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-117                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-118                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-119           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-120                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-14                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-121                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-122           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-123                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-124                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-125           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-126                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-127                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-128           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-129                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-15                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-130                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-131           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-132                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-133                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-134           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-135                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-136                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-137           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-138                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-16                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-139                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-140           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-141                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-142                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-143           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-144                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-145                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-146           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-147                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-17                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-148                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-149           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-150                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-151                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-152           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-153                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-154                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-155           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-156                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-18                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-157                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-158           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-159                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-160                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-161           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-162                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-163                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-164           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-165                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-19                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-166                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-167           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-168                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-169                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-170           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-171                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-172                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-173           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-174                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-20                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-175                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-176           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-177                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-178                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-179           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-180                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-181                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-182           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-183                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-21                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-184                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-185           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-186                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-187                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-188           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-189                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-190                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-191           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-192                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-22                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-193                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-194           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-195                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-196                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-197           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-198                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-199                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-200           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-201                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-23                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-202                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-203           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-204                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-205                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-206           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-207                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-208                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-209           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-210                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-24                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-211                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-212           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-213                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-214                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-215           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-216                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-217                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-218           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-219                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-25                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-220                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-221           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-222                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-223                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-224           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-225                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-226                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-227           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-228                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-26                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-229                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-230           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-231                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-232                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-233           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-234                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-235                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-236           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-237                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-27                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-238                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-239           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-240                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-241                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-242           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-243                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-244                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-245           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-246                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-28                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-247                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-248           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-249                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-250                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-251           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-252                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-253                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-254           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-255                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-29                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-256                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-257           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-258                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-259                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-260           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-261                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-262                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-263           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-264                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-30                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-265                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-266           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-267                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-268                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-269           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-270                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-271                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-272           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-273                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-31                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-274                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-275           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-276                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-277                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-278           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-279                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-280                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-281           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-282                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-32                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-283                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-284           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-285                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-286                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-287           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-288                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-289                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-290           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-291                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-33                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-292                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-293           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-294                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-295                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-296           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-297                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-298                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-299           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-300                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-34                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-301                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-302           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-303                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-304                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-305           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-306                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-307                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-308           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-309                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-35                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-310                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-311           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-312                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-313                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-314           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-315                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-316                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-317           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-318                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-36                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-319                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-320           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-321                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-322                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-323           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-324                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-325                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-326           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-327                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-37                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-328                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-329           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-330                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-331                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-332           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-333                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-334                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-335           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-336                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-38                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-337                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-338           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-339                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-340                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-341           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-342                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-343                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-344           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-345                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-39                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-346                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-347           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-348                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-349                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-350           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-351                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-352                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-353           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-354                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-40                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-355                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-356           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-357                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-358                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-359           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-360                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-361                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-362           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-363                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-41                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-364                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-365           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-366                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-367                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-368           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-369                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-370                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-371           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-372                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-42                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-373                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-374           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-375                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-376                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-377           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-378                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-379                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-380           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-381                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-43                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-382                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-383           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-384                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-385                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-386           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-387                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-388                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-389           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-390                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-44                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-391                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-392           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-393                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-394                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-395           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-396                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-397                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-398           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-399                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-45                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-400                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-401           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-402                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-403                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-404           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-405                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-406                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-407           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-408                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-46                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-409                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-410           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-411                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-412                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-413           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-414                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-415                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-416           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-417                  [1, 1024, 14, 14]         --\n",
      "│    └─Bottleneck: 2-47                  [1, 1024, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-418                [1, 256, 14, 14]          262,144\n",
      "│    │    └─BatchNorm2d: 3-419           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-420                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-421                [1, 256, 14, 14]          589,824\n",
      "│    │    └─BatchNorm2d: 3-422           [1, 256, 14, 14]          512\n",
      "│    │    └─ReLU: 3-423                  [1, 256, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-424                [1, 1024, 14, 14]         262,144\n",
      "│    │    └─BatchNorm2d: 3-425           [1, 1024, 14, 14]         2,048\n",
      "│    │    └─ReLU: 3-426                  [1, 1024, 14, 14]         --\n",
      "├─Sequential: 1-8                        [1, 2048, 7, 7]           --\n",
      "│    └─Bottleneck: 2-48                  [1, 2048, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-427                [1, 512, 14, 14]          524,288\n",
      "│    │    └─BatchNorm2d: 3-428           [1, 512, 14, 14]          1,024\n",
      "│    │    └─ReLU: 3-429                  [1, 512, 14, 14]          --\n",
      "│    │    └─Conv2d: 3-430                [1, 512, 7, 7]            2,359,296\n",
      "│    │    └─BatchNorm2d: 3-431           [1, 512, 7, 7]            1,024\n",
      "│    │    └─ReLU: 3-432                  [1, 512, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-433                [1, 2048, 7, 7]           1,048,576\n",
      "│    │    └─BatchNorm2d: 3-434           [1, 2048, 7, 7]           4,096\n",
      "│    │    └─Sequential: 3-435            [1, 2048, 7, 7]           2,101,248\n",
      "│    │    └─ReLU: 3-436                  [1, 2048, 7, 7]           --\n",
      "│    └─Bottleneck: 2-49                  [1, 2048, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-437                [1, 512, 7, 7]            1,048,576\n",
      "│    │    └─BatchNorm2d: 3-438           [1, 512, 7, 7]            1,024\n",
      "│    │    └─ReLU: 3-439                  [1, 512, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-440                [1, 512, 7, 7]            2,359,296\n",
      "│    │    └─BatchNorm2d: 3-441           [1, 512, 7, 7]            1,024\n",
      "│    │    └─ReLU: 3-442                  [1, 512, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-443                [1, 2048, 7, 7]           1,048,576\n",
      "│    │    └─BatchNorm2d: 3-444           [1, 2048, 7, 7]           4,096\n",
      "│    │    └─ReLU: 3-445                  [1, 2048, 7, 7]           --\n",
      "│    └─Bottleneck: 2-50                  [1, 2048, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-446                [1, 512, 7, 7]            1,048,576\n",
      "│    │    └─BatchNorm2d: 3-447           [1, 512, 7, 7]            1,024\n",
      "│    │    └─ReLU: 3-448                  [1, 512, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-449                [1, 512, 7, 7]            2,359,296\n",
      "│    │    └─BatchNorm2d: 3-450           [1, 512, 7, 7]            1,024\n",
      "│    │    └─ReLU: 3-451                  [1, 512, 7, 7]            --\n",
      "│    │    └─Conv2d: 3-452                [1, 2048, 7, 7]           1,048,576\n",
      "│    │    └─BatchNorm2d: 3-453           [1, 2048, 7, 7]           4,096\n",
      "│    │    └─ReLU: 3-454                  [1, 2048, 7, 7]           --\n",
      "├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n",
      "├─Linear: 1-10                           [1, 1000]                 2,049,000\n",
      "==========================================================================================\n",
      "Total params: 60,192,808\n",
      "Trainable params: 60,192,808\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 11.51\n",
      "==========================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 360.87\n",
      "Params size (MB): 240.77\n",
      "Estimated Total Size (MB): 602.25\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchinfo import summary\n",
    "\n",
    "model = torchvision.models.resnet152()\n",
    "print(summary(model, (1,3,224,224), depth=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98d31cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe5c030065c490eb2b65084ff29422e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f826a2076947f39b0cd9df17762b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/954 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset xsum/default (download: 245.38 MiB, generated: 507.60 MiB, post-processed: Unknown size, total: 752.98 MiB) to /home/xyin/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5b6cb1b36747deb52103c0ab3c4604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d0cc660a2e4ebe8c2f0af9539a807f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3e0ba55f544a4b94376ec08525ea8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.00M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset xsum downloaded and prepared to /home/xyin/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11bc51fef7c416cb3b51c83e151291d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "324c15a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyin/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56e976eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "with tokenizer.as_target_tokenizer():\n",
    "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))\n",
    "#{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0c62b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f1d7389",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fe6f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bee6bffe56448ab7a712e27ce54740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f880c5dcf8284536a70e67b8918a7602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94c9726a08a8483291777d24d300c8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd504d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40bd8962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "batch_size = 16\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"test-summarization\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,#至多保存3个模型\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8929ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5bfebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds] #按句子分割后换行符拼接\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8ebf24d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ee6e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/xyin/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 204045\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 12753\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.79 GiB total capacity; 5.88 GiB already allocated; 299.12 MiB free; 6.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1498\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1495\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1496\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1497\u001b[0m )\n\u001b[0;32m-> 1498\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1499\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:1740\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1738\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1740\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1743\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1746\u001b[0m ):\n\u001b[1;32m   1747\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1748\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:2470\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2470\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2472\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2473\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/trainer.py:2502\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2500\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2501\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2502\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2504\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1602\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[0;32m-> 1602\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[1;32m   1612\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   1613\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   1614\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1615\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1616\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:1035\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1023\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1024\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m     )\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1035\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:666\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    664\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    676\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:572\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    562\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    563\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    570\u001b[0m ):\n\u001b[1;32m    571\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 572\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    582\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/transformers/models/t5/modeling_t5.py:532\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    529\u001b[0m         position_bias \u001b[38;5;241m=\u001b[39m position_bias \u001b[38;5;241m+\u001b[39m mask  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    531\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias\n\u001b[0;32m--> 532\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtype_as(\n\u001b[1;32m    533\u001b[0m     scores\n\u001b[1;32m    534\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    535\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(\n\u001b[1;32m    536\u001b[0m     attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining\n\u001b[1;32m    537\u001b[0m )  \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:1818\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1816\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1817\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1818\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1820\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 7.79 GiB total capacity; 5.88 GiB already allocated; 299.12 MiB free; 6.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80c21f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a27d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08183932304382324,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading config.json",
       "rate": null,
       "total": 998,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16166ea3f0e84e4db52eb2c8226d1597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03317618370056152,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "Downloading pytorch_model.bin",
       "rate": null,
       "total": 1334448817,
       "unit": "B",
       "unit_divisor": 1024,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee13812749f4e97895b030cbfb0fedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForTokenClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(28996, 1024, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 1024)\n",
      "      (token_type_embeddings): Embedding(2, 1024)\n",
      "      (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (12): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (13): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (14): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (15): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (16): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (17): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (18): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (19): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (20): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (21): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (22): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (23): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "              (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "            (LayerNorm): LayerNorm((1024,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=1024, out_features=9, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import generic wrappers\n",
    "from transformers import AutoModel, AutoTokenizer, BertForTokenClassification \n",
    "\n",
    "\n",
    "# Define the model repo\n",
    "model_name = \"bert-base-uncased\" \n",
    "\n",
    "\n",
    "# Download pytorch model\n",
    "#model = AutoModel.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e670b26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (intermediate_act_fn): GELUActivation()\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cf3c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
